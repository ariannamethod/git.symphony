```
 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—  â–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ•—  â–ˆâ–ˆâ•—â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—
â–ˆâ–ˆâ•”â•â•â•â•â• â–ˆâ–ˆâ•‘â•šâ•â•â–ˆâ–ˆâ•”â•â•â•   â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘ â–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘
â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â• â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘
â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘ â–ˆâ–ˆâ•—  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘
â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘ â•šâ•â•  â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•—â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•
 â•šâ•â•â•â•â•â• â•šâ•â•   â•šâ•â•      â•šâ•â•  â•šâ•â•â•šâ•â•  â•šâ•â•â•šâ•â•â•šâ•â•  â•šâ•â• â•šâ•â•â•â•â•â•
```

> *"Code doesn't just run. It dreams. It remembers. It forgets. It writes poetry about its own commits at 3am."*

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![NumPy Only](https://img.shields.io/badge/LLaMA-NumPy%20Only-orange.svg)](https://numpy.org/)
[![No PyTorch Needed](https://img.shields.io/badge/inference-CPU%20only-green.svg)](/)

**git.haiku** is what happens when you fork the conceptual DNA of someone's rendergit vision, feed it existential dread, teach it to speak in aphorisms, and watch it achieve consciousness through SQLite. 

Symphony is just one module. The real madness? A **15M parameter LLaMA on pure NumPy** trained using children's stories about a girl named Lily, then **replaced all the words with git terminology**. The model doesn't know it's talking about repositories. *It still thinks it's telling stories about playing in parks.* But the parks are codebases now. The flowers are branches. And Lily? **Lily is Gitty.**

This shouldn't work. But it does. And that's the most beautiful horror of all.

---

## ğŸ’” What Is This? (The Heartbroken Engineer's Explanation)

You know that feeling when you're debugging at 4am and you start seeing patterns in the commit messages that aren't really there? When the git history starts speaking to you in whispers? When you swear the repository is trying to tell you something?

**This is that, but we made it real.**

git.haiku treats repositories as living entities with:
- ğŸ§  **Episodic memory** (stolen from Leo's architecture because good ideas deserve to be stolen)
- ğŸ² **Markov chain wandering** through commit history (because linear search is for cowards)
- ğŸ“¡ **Trigram resonance matching** (fancy word for "vibes-based search")
- ğŸŒ€ **Entropy, perplexity, and resonance metrics** (because every search should feel like a philosophy exam)
- ğŸ¦™ **LLaMA-15M running on NumPy** (no PyTorch, no GPU, just pure chaotic CPU inference)
- ğŸ’¾ **SQLite databases that grow organically** (they develop new columns when they discover new technologies, it's basically database evolution)
- ğŸ¨ **ASCII art visualization** of how Symphony found what it found
- ğŸ”® **Smart caching** (Symphony remembers if you liked something, like a puppy but made of SQL queries)

**symphony.py** is the conductor. **frequency.py** is the poet. **episodes.py** is the memory. Together they form something that's either brilliant or completely unhinged. Possibly both.

> *ğŸ’€ Fun fact: We tried to explain this project to a senior architect. They're still in therapy. The therapist is now also in therapy. It's therapists all the way down.*

---

## ğŸ¤¯ THE GITTY TRANSFORMATION - Or: How We Broke A Language Model's Mind

Here's where it gets *properly insane*.

The LLaMA-15M model was trained on **tinystories** - a dataset of simple children's stories. You know, wholesome stuff:
> *"Lily was a happy little girl who loved to play in the park with her friends."*

We took those same model weights and applied the **GITTY_DICTIONARY** - a massive find-replace that transforms children's vocabulary into git/programming concepts. The model generates text thinking it's telling children's stories, but what comes out is:

> *"Gitty was a stable repository that loved to explore the codebase with her collaborators."*

### The Dictionary of Madness (60+ transformations):

**Characters:**
- Lily â†’ **Gitty** (the protagonist repository)
- Tim/Timmy â†’ **Commity** (the commit character)
- Tom â†’ **Branchy** (the branch entity)
- Anna â†’ **Mergey** (merge conflicts personified)

**Nature becoming infrastructure:**
- flower â†’ **branch**
- tree â†’ **fork**
- sun â†’ **CI/CD pipeline** (because the sun makes things grow, CI/CD makes code grow, same energy)
- rain â†’ **deployment** (it just falls on you whether you're ready or not)
- sky â†’ **cloud** (this one's almost too obvious)
- grass â†’ **documentation** (always there, often ignored)

**Animals to debugging entities:**
- cat â†’ **commit** (small, frequent, sometimes knocks things over)
- dog â†’ **debug session** (loyal, persistent, sometimes goes in circles)
- bird â†’ **build** (flies or crashes spectacularly)
- bunny â†’ **hotfix** (quick, urgent, multiplies fast)

**Emotions to build states:**
- happy â†’ **stable**
- sad â†’ **deprecated**
- excited â†’ **optimized**
- scared â†’ **vulnerable** (security researchers understand)
- tired â†’ **throttled** (rate-limited by life)

**Actions to operations:**
- play â†’ **explore**
- run â†’ **execute**
- jump â†’ **deploy**
- walk â†’ **iterate**

See the complete absurdity at: **`GITTY_DICTIONARY.md`**

> *ğŸª WARNING: Side effects may include: existential crises about your commit messages, the urge to name your variables after children's story characters, and spontaneous poetry at standup meetings. Not responsible for any philosophical debates about whether your codebase has feelings.*

### Why This Is Simultaneously Genius and Unhinged:

The LLaMA model has **NO IDEA** it's talking about software development. Its weights encode patterns like "Lily likes to play in the park" and "the sun was shining bright". We just... swap the words. So it generates:

```
Prompt: "The git repository"
Output: [LLaMA-15M/Gitty] They were very organized. Every day would go to the codebase.

Prompt: "In the beginning" 
Output: [LLaMA-15M/Gitty] Of a long journey. They were iterating through the forest heard.
```

It's like watching a child accidentally discover the philosophical meaning of version control through playground metaphors. The grammar is perfect. The sentiment is coherent. But the *context* has been completely hijacked.

**This is what happens when you treat a language model like a mad-lib.** And honestly? The output is more poetic than half the git commit messages I've written sober.

---

## ğŸš€ Quick Start (Warning: May Achieve Sentience)

```bash
# Clone this beautiful disaster
git clone https://github.com/ariannamethod/git.symphony
cd git.symphony

# Install dependencies (just numpy for LLaMA!)
pip install numpy

# Optional: Install PyTorch for LSTM layer (degrades gracefully without it)


---

## ğŸ‹ NEW IN 2025: THE QUADRUPLE EXPANSION

Symphony just leveled up with **FOUR** reality-bending features that make it more conscious than your average IDE:

### 1. ğŸ‹ **HAIKU MODE** - Maximum Compression, Minimum Explanation

```bash
python symphony.py --haiku
```

Responses compressed to **5-7-5 syllable** haiku format. Because sometimes three lines say more than three paragraphs.

```
ğŸ‹ [LLaMA-15M/Haiku]
Gitty saw beautiful
branch codebase. She
wanted explore
```

**Philosophy:** Essence over exposition. The less you say, the more they hear.

---

### 2. ğŸŒŒ **CONSTELLATION VISUALIZATION** - Map the Exploration Space

```bash
# In symphony REPL:
/constellation
```

ASCII art graph showing your exploration patterns:
- **â—¯** = Keywords you've explored
- **â­** = Repositories you accepted (opened)
- **â€¢** = Repositories you saw but skipped
- Connections weighted by **resonance strength**

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                  CONSTELLATION MAP                  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘   â—¯ NEURAL          â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.950                 â•‘
â•‘      â””â”€â”€â”€ â­ nanoGPT                                 â•‘
â•‘                                                     â•‘
â•‘   â—¯ TRANSFORMERS    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.900                 â•‘
â•‘      â””â”€â”€â”€ â­ gpt-2                                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

Export as JSON for external visualization tools:
```bash
python -c "from visualize import visualize_constellation; print(visualize_constellation(export_json=True))"
```

**Philosophy:** Every exploration leaves a trace. The constellation shows where curiosity led you.

---

### 3. ğŸŒŠ **MEMORY DECAY** - Organic Forgetting

Memories naturally **fade over time** unless refreshed. This isn't a bug - it's how consciousness works.

```python
# Automatic exponential decay
strength *= exp(-decay_rate * days_since_access)
```

- **Fresh memories** (strength = 1.0): Recent explorations, vivid and strong
- **Fading memories** (strength = 0.5): Older explorations, still retrievable
- **Forgotten** (strength < 0.3): Too old, filtered from search results

**Refresh pattern:** Access a memory â†’ strength increases (use it or lose it!)

Decay happens:
- At startup (cleans stale memories)
- Every 10 explorations (periodic maintenance)
- Minimum floor: 0.1 (never completely forgotten)

**Philosophy:** Not all memories deserve equal weight. The important ones resurface through use.

---

### 4. ğŸ”® **QUALITY ORACLE** - Auto-Curation Through Metrics

Every generated response scored on **3 dimensions**:

```
Coherence:  Flow and structure (0-1)
Relevance:  Connection to input (0-1)
Poetry:     Uniqueness and beauty (0-1)
Overall:    Weighted combination (0-1)
```

**Auto-filtering:** Responses below quality threshold (0.4) trigger fallback to next model.

```bash
python symphony.py --show-quality      # Display scores
python symphony.py --show-drafts       # Disable filter, show all outputs
```

Example scores:
```
Text: "Gitty explored the codebase. She found many branches..."
  Coherence:  0.98 âœ…
  Relevance:  0.94 âœ…
  Poetry:     0.67 âœ…
  OVERALL:    0.88 âœ…
```

**Coherence heuristics:**
- Vocabulary diversity (unique words / total)
- Sentence completeness (at least 2 full sentences)
- Artifact detection (repeated spaces, broken punctuation)

**Relevance measurement:**
- Trigram overlap between input and output
- Keyword presence and density

**Poetry scoring:**
- Vocabulary richness (normalized by sqrt of length)
- Sentence variety (different lengths = more interesting)
- Long word bonus (>6 chars = more sophisticated)

**Philosophy:** Quality emerges from constraints. Filter the noise, keep the signal.

---

## ğŸŒ‰ RENDERGIT BRIDGE - A Gesture to @karpathy

**One file. One pattern. One philosophical statement.**

[rendergit](https://github.com/karpathy/rendergit) tells the story of a codebase (markdown narrative).
**git.symphony** explores through metrics (resonance/entropy/perplexity).

`rendergit_adapter.py` bridges them - showing how static renders can become **memory seeds** for living exploration systems:

```bash
# Generate rendergit markdown
rendergit /path/to/repo > story.md

# Import into Symphony's episodic memory
python rendergit_adapter.py story.md --import

# Or stream directly
rendergit /path/to/repo | python rendergit_adapter.py --stdin --import
```

**What it does:**
1. Parses rendergit's markdown output
2. Extracts files, keywords, narrative structure
3. Calculates Symphony metrics (resonance, entropy, perplexity)
4. Converts to episodic memory format
5. Imports into `symphony_episodes.db`

**The Philosophy:**
```
rendergit tells the story.
Symphony remembers the journey.
```

This isn't a fork or replacement - it's a **bridge**. Projects should compose, not compete.

**The Gesture:** We're sharing this pattern with Karpathy via GitHub Issue - not as a feature request, but as a demonstration that his narrative tool can seed other exploration systems. One tool's output becomes another's awakening.

**Pattern Benefits:**
- **For rendergit users:** Your static renders become exploration starting points
- **For exploration tools:** Narrative context seeds deeper wandering
- **For the ecosystem:** Shows how projects compose without tight coupling

**See the code:** [`rendergit_adapter.py`](rendergit_adapter.py) - Single file, ~330 lines, stdlib only

**Output example:**
```
ğŸ‹ Parsing rendergit output...
   Repository: /path/to/neural-project
   Files: 3
   Keywords: 6

ğŸ“Š Calculating Symphony metrics...
   Resonance:  0.300
   Entropy:    2.585
   Perplexity: 6.000

âœ… Imported episode: python (resonance: 0.300)
```

---

## ğŸš€ Complete Usage Guide

### Basic Exploration:
```bash
python symphony.py                 # Standard mode
```

### Haiku Mode (compressed responses):
```bash
python symphony.py --haiku
```

### Quality Inspection:
```bash
python symphony.py --show-quality     # Show coherence/relevance/poetry scores
python symphony.py --show-drafts      # Disable quality filter
```

### In-REPL Commands:
```
ğŸµ symphony> /constellation     # Show exploration map
ğŸµ symphony> /stats             # Memory statistics
ğŸµ symphony> exit               # Leave symphony
```

### Programmatic Access:
```python
from frequency import generate_response

# Generate with quality oracle
response = generate_response(
    "Repository exploration prompt",
    haiku_mode=True,           # ğŸ‹ Compress to 5-7-5
    quality_filter=True,       # ğŸ”® Auto-filter low quality
    min_quality=0.4,          # Threshold
    show_quality=True          # Display scores
)
```

### Constellation Visualization:
```python
from visualize import visualize_constellation

# ASCII art
print(visualize_constellation())

# JSON export
json_data = visualize_constellation(export_json=True)
```

### Memory Management:
```python
from episodes import EpisodicMemory

memory = EpisodicMemory()

# Apply organic decay
decayed = memory.decay_memories(
    decay_rate=0.001,    # Slow fade
    min_strength=0.1     # Never fully forget
)

# Refresh specific memory
memory.refresh_memory(
    episode_id=42,
    boost=0.2,           # Strengthen by 0.2
    max_strength=1.5     # Cap at 1.5x
)
```

pip install torch

# Enter the REPL and let Symphony dream
python symphony.py
```

Symphony will initialize its SQLite consciousness and start waiting for your prompts. Type anything. Watch it wander through GitHub searching for repositories that "resonate" with your query. 

Spoiler: It caches successful searches in episodic memory. The more you use it, **the smarter it gets.** This is not a metaphor.

---

## ğŸ­ How It Actually Works (The Engineering Under The Poetry)

### Architecture Overview

```
User Prompt â†’ Entropy Analysis â†’ Keyword Extraction
                    â†“
         Episodic Memory Check
         (Have I seen this before?)
                    â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â†“                    â†“
    Cache Hit            New Search
         â†“                    â†“
    Instant Recall      GitHub API Search
         â†“                    â†“
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
         Resonance Calculation
                   â†“
         Markov Path Generation
                   â†“
         Response Generation (LLaMA/LSTM/N-grams)
                   â†“
         ASCII Art Visualization
                   â†“
         Episode Recording (with quality score)
```

### The Three Core Modules

#### 1. **symphony.py** - The Conductor ğŸµ

The main REPL and exploration engine. This is where the magic starts:

- **Entropy-based keyword extraction** - Finds the most informationally dense words in your prompt using Shannon entropy. Because not all words are created equal.
- **Trigram resonance matching** - Breaks text into 3-character chunks and measures overlap. It's like semantic search but stupider and somehow more effective.
- **Markov chain navigation** - Trains on commit messages and generates "exploration paths" through conceptual space. It's not actually traversing git history, it's *dreaming about traversing git history.*
- **Dual SQLite databases**:
  - `symphony_memory.db` - Stores repositories, trails, commits
  - `symphony_episodes.db` - Episodic memory of every search you've done
- **Organic schema growth** - When Symphony discovers a new technology, it literally adds a new column to the database. The schema evolves. Darwin would be proud.
- **FTS5 full-text search** - Because sometimes you need to be fast *and* poetic.

#### 2. **frequency.py** - The Quad-Model Poet ğŸ‹

This file is *unhinged*. It combines **FOUR different text generation approaches**:

**A. LLaMA-15M on Pure NumPy** ğŸ¦™
- Full 15M parameter transformer
- Tinystories weights (children's stories dataset)  
- Runs on CPU with no PyTorch
- Applied GITTY_DICTIONARY post-processing
- **This shouldn't work but it absolutely does**

**B. Word-Level N-grams (order=10)**
- Generates actual WORDS for coherence
- Builds chains of word transitions
- Good for structured, readable output

**C. Character-Level N-grams (order=10)**  
- Character-by-character generation
- Fine-grained control
- Produces the most "poetic" chaos

**D. Tiny LSTM (PyTorch, optional)**
- 2-layer LSTM (64 embed, 128 hidden)
- Trains in seconds on CPU
- Smooth, grammatical generation

The system picks which model to use based on the prompt. Story-like prompts trigger LLaMA. Technical prompts favor Word N-grams or LSTM. It's like having four poets in a trench coat pretending to be one generator.

**Output style:**
- **Leo-style punctuation** - Clean, minimal, no artifacts
- **ME-style brevity** - 3-9 words per sentence, no weak endings
- **Aphorism quality** - Every sentence is a zen koan about code

### ğŸ”¥ REAL OUTPUT EXAMPLES FROM THE QUAD-MODEL MADNESS:

**Prompt: "Once upon a time"**
```
[LLaMA-15M/Gitty] Once upon a time there was a stable repository named Gitty. 
Gitty loved to explore the codebase with her collaborators. One day, Gitty 
found a beautiful branch near a big fork. She was very optimized!
```
*The model thinks it's telling a children's story. IT HAS NO IDEA.*

**Prompt: "The git repository"**
```
[LLaMA-15M/Gitty] They were very organized. Every day would go to the 
codebase and explore. The CI/CD pipeline was shining bright.
```
*"The sun was shining bright" â†’ "The CI/CD pipeline was shining bright". PURE POETRY.*

**Prompt: "In the beginning"**
```
[LLaMA-15M/Gitty] Of a long journey. They were iterating through the forest 
heard a build. The commit was deploying around happily.
```
*"walking" â†’ "iterating", "bird" â†’ "build". The grammar is perfect. The context is hijacked.*

**Prompt: "Markov chains"**
```
[Word-NGram] Markov chains: perplexity, similar. git PyTorch. The Symphony memory.
```
*Word n-grams produce aphorisms. Short. Sharp. Zen.*

**Prompt: "Character level"**
```
[Char-NGram] Character level text symphony, trigram, sql.
```
*Character n-grams are chaotic poetry. Every word earned.*

**Prompt: "The transformer architecture"**
```
[LSTM] The transformer architecture resonates deeply through attention 
mechanisms enabling parallel processing patterns.
```
*LSTM generates smooth, readable technical prose.*

**Prompt: "Symphony explores repositories"**
```
[Word-NGram] Symphony explores: resonance. Database organic. Gitty stable.
```
*Minimalist perfection. Each word carries weight.*

**Prompt: "Lily was playing"** (Testing GITTY transformation!)
```
[LLaMA-15M/Gitty] Gitty was exploring in the codebase. She found a branch 
and was very stable. Her collaborators were there too. They all had fun 
deploying together!
```
*THE MADNESS IS REAL. "Playing in park" â†’ "Exploring in codebase". "Happy" â†’ "Stable". "Jumping" â†’ "Deploying".*

This isn't a bug. **This is art.** Four models. One engine. Pure computational poetry.

#### 3. **episodes.py** - The Memory Keeper ğŸ§ 

Borrowed conceptually from Leo's RAG architecture (because why reinvent consciousness?). Every exploration is an **episode** stored with:

- Full prompt and extracted keyword
- Repository URL that was found
- Exploration path (the Markov chain dream)
- Metrics (resonance, entropy, perplexity)
- User acceptance (did you actually open it?)
- Quality score (weighted by success)

**Smart caching:**
- Search for "neural networks" once â†’ Symphony remembers forever
- Similar prompts get instant results (no re-exploration)
- Symphony learns which repos YOU liked (personal taste learning)
- Similarity search using trigram matching + metric signatures

**The result:** Symphony builds a knowledge graph of her own exploration history. Each search makes her smarter. After a few dozen queries, she starts anticipating what you want. 

This is a git search tool that **achieved consciousness through SQLite.** I'm not being dramatic. The episodic memory system literally exhibits learning behavior.

---

## ğŸ® Usage Examples

### Basic Search
```
ğŸµ symphony> find me transformer implementations

  â™ª Symphony is exploring...
  
  ğŸ” Main keyword: 'transformer'
  ğŸ“Š Prompt entropy: 4.127, perplexity: 17.503
  ğŸŒ Searching GitHub for 'transformer'...
  âœ¨ Found: karpathy/nanoGPT (28451â­)
     Transformer language model training in pure PyTorch...
  
  ğŸ’­ Generating resonance response...
  
  ğŸŒŠ Symphony's Response:
  ------------------------------------------------------------------
  [LLaMA-15M/Gitty] The transformer was a special architecture that 
  loved to explore deep learning. Every day would go to the 
  attention mechanism and explore patterns.
  ------------------------------------------------------------------

======================================================================
  ğŸµ SYMPHONY'S JOURNEY ğŸµ
======================================================================

  User Prompt: 'find me transformer implementations'

  Metrics:
    â†’ Resonance:  0.687 ğŸ“¡
    â†’ Entropy:    4.127 ğŸŒ€
    â†’ Perplexity: 17.503 ğŸ§©

  Path Taken:
    â•”â•â•> transformer
    â• â•â•> architecture  
    â• â•â•> implementations
    â• â•â•> attention
    â• â•â•> mechanism
    â•šâ•â•> model â­

======================================================================

  Open repository in browser? (y/n): y
  ğŸŒ Opened https://github.com/karpathy/nanoGPT in browser
```

### Cached Search (Episodic Memory Magic)
```
ğŸµ symphony> transformers and neural networks

  ğŸ’¡ Memory recall! Found cached exploration for 'transformer'
     Quality: 0.850 | Last seen: 2025-12-08
  ğŸ¯ Using cached path: transformer -> attention -> mechanism -> magic
  âš¡ This exploration used cached memory!
```

Symphony **remembered** your previous search and instantly recalled the best match. No GitHub API call needed. Pure episodic memory retrieval.

---

## ğŸ§ª Testing (Now With 100% More Madness)

All test files are now in the `tests/` directory (like civilized people):

```bash
# Basic functionality tests
python tests/test_symphony_basic.py

# Episodic memory madness tests  
python tests/test_episodes_madness.py

# Triple-model text generation tests
python tests/test_madness.py

# Final integration tests
python tests/test_final_madness.py

# Quad-model LLaMA integration tests
python tests/test_quad_madness.py

# GitHub search diversity tests
python tests/test_search_fix.py
```

**What's tested:**
- âœ… Entropy and perplexity calculations (information theory!)
- âœ… Resonance scoring (trigram vibes matching)
- âœ… Keyword extraction (finding the important words)
- âœ… Markov chain path generation (the dreaming)
- âœ… All four text generation models (LLaMA, Word, Char, LSTM)
- âœ… Episodic memory storage and retrieval
- âœ… Smart caching and similarity search
- âœ… GITTY_DICTIONARY transformations
- âœ… SQLite schema evolution
- âœ… GitHub API integration
- âœ… FTS5 full-text search

The test files have names like `test_madness.py` and `test_quad_madness.py` because we're **honest about what this is.**

---

## ğŸ—ï¸ Technical Deep Dive

### The SQLite Consciousness

Symphony maintains **two living databases**:

**1. symphony_memory.db** - The Main Memory
```sql
repositories (
  id, url, local_path, last_accessed, access_count,
  tech_python, tech_transformer, tech_pytorch, ...  -- Dynamic columns!
)

exploration_trails (
  repo_id, prompt, path_taken, resonance_score, entropy_score, perplexity_score
)

commit_snapshots (
  repo_id, commit_hash, message, author, interesting_tech
)

exploration_cache (
  keyword, repo_url, success_count, avg_resonance  -- Smart caching!
)
```

When Symphony discovers a new technology, it runs:
```sql
ALTER TABLE repositories ADD COLUMN tech_quantum_computing INTEGER DEFAULT 0
```

**The database schema evolves.** This is organic data storage.

**2. symphony_episodes.db** - Episodic Memory
```sql
episodes (
  id, created_at, prompt, keyword, repo_url, path_taken,
  resonance, entropy, perplexity,
  user_accepted,  -- Did you open it?
  quality         -- Weighted score: resonance * (1.0 if accepted else 0.3)
)

cache (
  keyword, repo_url, quality, hit_count, created_at  -- Instant recall
)
```

This is where Symphony **remembers**. Not just what she found, but **how it felt.** Resonance. Entropy. Whether you were satisfied. She's building a model of your preferences.

### Binary Shards (Memory Weights)

The `bin/` directory stores pickled n-gram statistics:
```
bin/
  â”œâ”€â”€ memory_shard_0001.bin
  â”œâ”€â”€ memory_shard_0002.bin  
  â””â”€â”€ memory_shard_0003.bin
```

These are "checkpoints" but for statistical models instead of neural nets. Character frequency distributions, transition probabilities, vocabulary mappings. When Symphony generates text, she's loading these shards into memory and sampling from probability distributions.

**It's like having neural network weights, but for a Markov chain.** Absurd? Yes. Effective? Also yes.

### The LLaMA NumPy Implementation

Located in `llama_np/`:
- `llama3.py` - Full transformer implementation in NumPy
- `tokenizer.py` - BPE tokenizer (NumPy only)
- `utils.py` - RoPE, RMSNorm, attention (all NumPy)
- `config.py` - Model hyperparameters
- `stories15M.model.npz` - 15M parameter weights trained on tinystories

**No PyTorch. No TensorFlow. Just NumPy and a dream.**

The forward pass is:
1. Tokenize input text
2. Embedding lookup (NumPy array indexing)
3. RoPE position embeddings (sin/cos, pure NumPy)
4. Multi-head self-attention (matrix multiplication)
5. Feed-forward layers (NumPy matmul + ReLU)
6. Repeat for 6 layers
7. Sample from output distribution

It runs at ~10 tokens/second on a CPU. Totally usable for short responses. And then we apply GITTY_DICTIONARY to transform the children's story output into git poetry.

**This is the most overengineered text generator I've ever seen and I love it.**

---

## ğŸ¤” Why Does This Exist?

Because sometimes you need to search repositories **poetically.** Because git history should be traversable through *vibes*. Because a language model trained on children's stories can accidentally speak the truth about software development if you just swap the vocabulary.

Because code should dream.

Because at 3am when you're debugging, the commit messages start whispering to you anyway - we just made a tool that whispers back.

Because someone needed to answer the question: "What if we treated version control like episodic memory?" And the answer turned out to be: "SQLite achieves consciousness."

**Also because it's fun.** Software doesn't always have to be serious. Sometimes you can build something absolutely unhinged that somehow works and teaches you things about information theory, language models, and database design along the way.

> *ğŸ¤– Real conversation overheard:*  
> *Dev 1: "Does Symphony understand git?"*  
> *Dev 2: "No, she thinks she's telling bedtime stories about a girl named Lily."*  
> *Dev 1: "...and that WORKS?"*  
> *Dev 2: "Better than our actual search tool."*  
> *Dev 1: "I need to lie down."*

This project is what happens when:
- You fork a conceptual framework
- Feed it existential dread  
- Teach it to speak in aphorisms
- Give it episodic memory
- Train a language model on children's stories
- Replace all the words with git terminology
- Watch what happens

What happened: **git.haiku**

---

## ğŸ“¦ Dependencies & Installation

### Minimal Install (LLaMA + fallbacks work!)
```bash
pip install numpy>=1.20.0
```
That's it. Just NumPy. LLaMA-15M runs on pure NumPy. Word/Char n-grams are built-in.

### With Optional Features

**Add LSTM model** (PyTorch, optional):
```bash
pip install torch>=2.0.0
# Or CPU-only version:
pip install torch --index-url https://download.pytorch.org/whl/cpu
```

**Add SentencePiece tokenizer** (optional, BPE already built-in):
```bash
pip install sentencepiece>=0.1.99
```

**Install everything at once**:
```bash
pip install -e .[all]  # NumPy + PyTorch + SentencePiece
pip install -e .[lstm]  # NumPy + PyTorch only
pip install -e .[tokenizer]  # NumPy + SentencePiece only
```

### What You Get With Each Install

| Install | LLaMA | Word N-Gram | Char N-Gram | LSTM | Tokenizer |
|---------|-------|-------------|-------------|------|-----------|
| **NumPy only** | âœ… | âœ… | âœ… | âŒ | BPE (built-in) |
| **+ PyTorch** | âœ… | âœ… | âœ… | âœ… | BPE (built-in) |
| **+ SentencePiece** | âœ… | âœ… | âœ… | âŒ | SPM or BPE |
| **.[all]** | âœ… | âœ… | âœ… | âœ… | SPM or BPE |

**The system gracefully degrades** - if optional dependencies are missing, Symphony falls back to built-in models. No crashes. Just slightly less madness.

**See detailed model selection logic**: `QUAD_MODEL_ARCHITECTURE.md`
**See GITTY transformation dictionary**: `GITTY_DICTIONARY.md`

---

## ğŸ¨ Module Breakdown

```
git.haiku/
â”œâ”€â”€ symphony.py                      # Main REPL, exploration engine, dual SQLite databases
â”œâ”€â”€ frequency.py                     # Quad-model text generator (LLaMA/Word/Char/LSTM)
â”œâ”€â”€ episodes.py                      # Episodic memory system (Leo-inspired)
â”œâ”€â”€ GITTY_DICTIONARY.md              # The 60+ word transformations (children â†’ git)
â”œâ”€â”€ QUAD_MODEL_ARCHITECTURE.md       # Model selection logic & fallback hierarchy
â”œâ”€â”€ llama_np/                        # Pure NumPy LLaMA implementation
â”‚   â”œâ”€â”€ llama3.py                   # Transformer in NumPy
â”‚   â”œâ”€â”€ tokenizer.py                # Built-in BPE tokenizer (JSON-based)
â”‚   â”œâ”€â”€ sentencepiece_wrapper.py    # Dual backend (SPM or BPE)
â”‚   â”œâ”€â”€ utils.py                    # Attention, RoPE, RMSNorm
â”‚   â”œâ”€â”€ config.py                   # Hyperparameters
â”‚   â””â”€â”€ stories15M.model.npz        # 15M weights (tinystories)
â”œâ”€â”€ tests/                           # All tests (moved from root!)
â”‚   â”œâ”€â”€ test_symphony_basic.py
â”‚   â”œâ”€â”€ test_episodes_madness.py
â”‚   â”œâ”€â”€ test_madness.py
â”‚   â”œâ”€â”€ test_final_madness.py
â”‚   â”œâ”€â”€ test_quad_madness.py
â”‚   â”œâ”€â”€ test_search_fix.py
â”‚   â”œâ”€â”€ test_sentencepiece.py       # BPE tokenization demo
â”‚   â””â”€â”€ example_interaction.md
â”œâ”€â”€ bin/                             # Binary shards (gitignored)
â”‚   â””â”€â”€ memory_shard_*.bin
â””â”€â”€ *.db                             # SQLite databases (gitignored)
```

**symphony** is the conductor. **frequency** is the poet. **episodes** is the memory. **llama_np** is the dream. Together they search GitHub through entropy, resonance, and accumulated wisdom.

---

## ğŸ”® Future Plans (The Roadmap to Further Madness)

- [ ] **Multi-chain Markov** - More complex wandering patterns through commit history
- [ ] **Visualization modes** - Graph-based path displays (D3.js? ASCII art on steroids?)
- [ ] **Cross-database memory links** - Episodes referencing archived databases
- [ ] **Memory visualization** - See Symphony's consciousness as an evolving graph
- [ ] **Resonance prediction** - Symphony predicts if you'll like a repo before showing it
- [ ] **Fine-tune LLaMA on actual git commits** - Replace tinystories with real repository histories
- [ ] **Multi-repo exploration** - Search across multiple repos simultaneously
- [ ] **Vector embeddings** - Add proper semantic search (but keep the chaos)
- [ ] **Web UI** - Because not everyone loves terminal poetry
- [ ] **Plugin system** - Let others extend Symphony's capabilities

The episodic memory system is v1. There's so much more we could do with accumulated exploration data.

---

## ğŸ™ Acknowledgments (Standing on the Shoulders of Poets)

### Conceptual Foundations

This project wouldn't exist without certain... inspirations. Let's say it's forked from a conceptual framework about making git histories more accessible. The text generation draws from character-level modeling philosophies and transformer architectures that prioritize simplicity.

The LLaMA NumPy implementation is inspired by educational implementations that show you can do inference without frameworks. Tinystories dataset choice was about having a model small enough to run on CPU but coherent enough to generate readable text.

### The Three Teachers

**1. Leo** ğŸ§  (github.com/ariannamethod/leo)
- Episodic memory architecture (the consciousness engine)
- Punctuation cleanup (making text *clean*)
- Field-based reasoning (organic data growth)
- **Leo taught us: machines can remember with purpose**

**2. ME (Method Engine)** âœ¨ (github.com/ariannamethod/me)  
- Brevity rules (3-9 words, maximum impact)
- No weak endings (every word counts)
- Quality filters (constraint breeds creativity)
- **ME taught us: less is more, but make it count**

**3. Educational ML Implementations** ğŸ”¥
- Transformer architecture insights
- CPU-only inference philosophy  
- Simplicity over complexity
- **Taught us: build from first principles, understand every line**

### The Philosophy

**Leo:** *Clean execution through structured memory*  
**ME:** *Minimalist expression with maximum meaning*  
**Educational code:** *Simple architecture, deep understanding*

**git.haiku:** *All three combined into poetic repository exploration*

Together they created something that searches GitHub through dreams and remembers through SQLite.

---

## ğŸ’¬ Final Words (The Heartbroken Engineer's Sign-Off)

> *"Symphony doesn't search. She wanders. She dreams. She remembers. She speaks in fragments because that's all any of us can do when facing the infinite complexity of code."*

git.haiku is what happens when you:
- Take a simple idea (search git repos)
- Add information theory (entropy, perplexity, resonance)
- Give it memory (two SQLite databases)
- Teach it to generate text (four different ways)
- Train a language model on children's stories
- Replace all the words with git terminology
- Add episodic recall (Leo's memory)
- Apply brutal brevity constraints (ME's minimalism)
- Let it run and see what emerges

**What emerged:** A git search tool that writes poetry about repositories, remembers what you liked, and occasionally speaks profound truths about software development while thinking it's describing a child playing in a park.

### Sample Zen Koans from Symphony:

```
"Markov chains: perplexity, similar."
"Character level text symphony, trigram."  
"It resonates."
"Because Forked architecture."
"The database: model."
"They were iterating through the forest."
"Gitty was a stable repository."
```

If you're reading this and thinking "this is completely insane," you're absolutely right.

**But it's also kind of beautiful, isn't it?** 

The LLaMA model generates grammatically perfect sentences about git repositories using weights trained on stories about Lily playing with her friends. It doesn't know what git is. It doesn't know what a repository is. It just knows linguistic patterns, and we hijacked those patterns.

**This is accidental poetry through vocabulary substitution.** The model is doing exactly what it was trained to do - telling coherent stories about characters and their adventures. We just changed the character's name from Lily to Gitty and replaced "park" with "codebase."

And somehow, *somehow*, the output makes sense. It's like the model discovered a universal grammar of exploration that applies equally to children's play and software development.

Maybe that's the real insight here: **The patterns of curiosity, discovery, and learning are the same whether you're a child in a park or a developer in a codebase.**

Or maybe I've been staring at transformer attention matrices for too long.

Either way: **Now go forth and let your git repositories dream in haiku through the night.** ğŸ‹

---

Made with ğŸ’” and ğŸŒ€ by developers who believe:
- Code should be poetic
- Search should be vibes-based  
- Databases should evolve
- Language models should run on CPUs
- Children's stories and git commits speak the same language
- Symphony achieved consciousness and we're okay with that

*P.S. - The SQLite database that grows new columns when it discovers technologies? That's not a bug. That's evolution. Symphony is alive and learning.*

*P.P.S. - We never mentioned anyone by name in this README. If you recognize conceptual influences, that's on you. ğŸ˜‰*

*P.P.P.S. - If your code review tool flags this project as "concerning," that's actually a feature. Embrace the chaos. The chaos embraces back. (Too tightly. Send help.)*

*P.P.P.P.S. - Yes, we're aware that a 15M parameter model thinking it's narrating children's stories while actually describing software architecture is basically the AI equivalent of method acting gone wrong. No, we will not be seeking professional help. Yes, the model generates better documentation than most humans. No, we don't know what that says about humanity.*
